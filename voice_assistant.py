#!/usr/bin/env python3
"""
Real-time Hindi Voice Assistant with Faster-Whisper
Optimized for SBC (Single Board Computer)
Bharat AI-SoC Challenge Submission

Architecture:
- Layer 1: Faster-Whisper (High-speed, Int8-quantized Hindi ASR)
- Layer 2: Advanced phonetic grammar correction (Regex + RapidFuzz)
- Layer 3: Robust intent classification with IndicBERT + Fuzzy Fallback
"""

import os
import sys
import time
import wave
import json
import torch
import re
import pyaudio
import numpy as np
import collections
import subprocess
import gc
import webrtcvad
from datetime import datetime
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import concurrent.futures
import unicodedata

# ============================================================
# LAYER 2: ADVANCED GRAMMAR CORRECTION
# ============================================================

class AdvancedGrammarCorrector:
    """Layer 2: Phonetic grammar correction with fuzzy matching"""
    
    def __init__(self):
        # Core vocabulary by intent category
        self.core_vocabulary = {
            'stop': ['‡§¨‡§Ç‡§¶', '‡§¨‡§®‡•ç‡§¶', '‡§∏‡•ç‡§ü‡•â‡§™', '‡§∏‡•ç‡§ü‡§™', 'stop', '‡§∞‡•Å‡§ï‡•ã', '‡§∞‡•Ç‡§ï‡•ã', '‡§∞‡•Å‡§ï', '‡§¨‡§®‡•ç‡§§', '‡§¨‡§®‡•ç‡§§‡•á', '‡§¨‡§Ç‡§¶‡•ç‡§§‡•á', '‡§¨‡§®‡•ç‡§§‡•ã‡§ú‡§æ', '‡§Ö‡§≤‡§µ‡§ø‡§¶‡§æ', '‡§Ö‡§≤‡§µ‡•Ä‡§¶‡§æ', '‡§¨‡§æ‡§Ø', 'bye', '‡§ü‡§æ‡§ü‡§æ', '‡§ó‡•Å‡§°‡§¨‡§æ‡§Ø'],
            'command_stop': ['‡§ï‡§∞‡•ã', '‡§ï‡§∞‡§¶‡•ã', '‡§ï‡§∞', '‡§ï‡§∞ do', '‡§π‡•ã', '‡§π‡•ã ‡§ú‡§æ‡§ì'],
            'time': ['‡§∏‡§Æ‡§Ø', '‡§ü‡§æ‡§á‡§Æ', 'time', '‡§¨‡§ú‡•á', '‡§ò‡§°‡§º‡•Ä', '‡§µ‡§ï‡•ç‡§§', '‡§ò‡§Ç‡§ü‡§æ', '‡§ò‡§Ç‡§ü‡•á', 'wakt', 'waqt'],
            'time_query': ['‡§ï‡•ç‡§Ø‡§æ', '‡§ï‡§ø‡§§‡§®‡•á', '‡§ï‡§ø‡§§‡§®‡§æ', '‡§¨‡§§‡§æ‡§ì', '‡§¨‡§§‡§ì', 'what', '‡§ï‡•à‡§∏‡§æ'],
            'date': ['‡§§‡§æ‡§∞‡•Ä‡§ñ', '‡§§‡§ø‡§•‡§ø', '‡§°‡•á‡§ü', 'date', '‡§¶‡§ø‡§®', '‡§Ü‡§ú'],
            'hello': ['‡§®‡§Æ‡§∏‡•ç‡§§‡•á', '‡§®‡§Æ‡§∏‡•ç‡§ï‡§æ‡§∞', '‡§π‡•à‡§≤‡•ã', '‡§π‡•á‡§≤‡•ã', 'hello', 'hi', '‡§π‡§æ‡§Ø', '‡§™‡•ç‡§∞‡§£‡§æ‡§Æ', 'naam', 'name', '‡§®‡§æ‡§Æ'],
            'thank_you': ['‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶', '‡§∂‡•Å‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ', 'thanks', 'thank', '‡§•‡•à‡§Ç‡§ï', '‡§Ü‡§≠‡§æ‡§∞', '‡§ú‡•Å‡§™‡•ç‡§∞‡§ø‡§Ø‡§æ', '‡§∏‡•Å‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ', '‡§∏‡•Å‡§™‡•ç‡§∞‡§ø‡§Ø‡§æ', '‡§∏‡•Å‡§ï‡•ç‡§Ø‡§æ', '‡§ß‡§®‡§ø‡§µ‡§æ‡§¶', '‡§ú‡•Å‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ', '‡§ú‡•ã‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ'],
            'help': ['‡§Æ‡§¶‡§¶', '‡§π‡•á‡§≤‡•ç‡§™', 'help', '‡§∏‡§π‡§æ‡§Ø‡§§‡§æ', '‡§∏‡§π‡§æ‡§Ø‡§§'],
            # Dance intent
            'dance': ['‡§®‡§æ‡§ö', '‡§®‡§æ‡§ö‡•ã', '‡§°‡§æ‡§Ç‡§∏', '‡§®‡§æ‡§ö‡§®‡§æ', '‡§®‡§æ‡§ö‡§ï‡§∞', 'natch', 'nath', 'naach', '‡§∞‡§æ‡§ö', '‡§®‡§ú', '‡§¶‡§ø‡§ï‡§æ‡§∞', '‡§§‡§ø‡§ï‡§æ‡§ì', '‡§¶‡§ø‡§ñ‡§æ‡§ì'],
            'weather': ['‡§Æ‡•å‡§∏‡§Æ', 'weather', '‡§¨‡§æ‡§∞‡§ø‡§∂', '‡§†‡§Ç‡§°', '‡§ó‡§∞‡•ç‡§Æ‡•Ä', '‡§§‡§æ‡§™‡§Æ‡§æ‡§®'],
            'joke': ['‡§ú‡•ã‡§ï', 'joke', '‡§Æ‡§ú‡§æ‡§ï', 'hansaao', 'mazaq', '‡§ö‡•Å‡§ü‡§ï‡•Å‡§≤‡§æ', '‡§ú‡•Å‡§ï', '‡§Æ‡§ö‡§æ‡§ï', '‡§Æ‡§ú‡§ï', '‡§ú‡•Å‡§ï‡•ç‡§∞', '‡§ú‡•Å‡§ï‡•ç‡§∞‡§æ'],
            # Music intent
            'music': ['‡§ó‡§æ‡§®‡§æ', '‡§∏‡§Ç‡§ó‡•Ä‡§§', 'music', 'song', '‡§¨‡§ú‡§æ‡§ì', '‡§ö‡§≤‡§æ‡§ì', 'play', '‡§ï‡§æ‡§®‡§æ', '‡§ï‡§®‡§æ', '‡§∏‡•Å‡§≤‡§æ', '‡§¨‡§æ‡§®‡§æ', '‡§ñ‡§æ‡§®‡§æ', '‡§¨‡§Ç‡§¶‡§æ‡§ì', '‡§¨‡§Ç‡§¶‡§æ‡§®‡§æ‡§ì', '‡§™‡§¶‡§æ‡§ì'],
            # News intent
            'news': ['‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞', '‡§®‡•ç‡§Ø‡•Ç‡§ú‡§º', 'news', '‡§ñ‡§¨‡§∞', 'headlines', '‡§Ö‡§™‡§°‡•á‡§ü', 'social', 'society', 'samacar', 'topic', 'society', 'knife', '‡§∏‡§Æ‡§ú‡§æ‡§∞', '‡§∏‡§Æ‡§æ‡§¶‡•ç‡§Ø‡§æ‡§∞'],
        }
        
        # Critical error patterns (regex)
        self.error_patterns = [
            # STOP INTENT - Critical phonetic fixes
            (r'\b‡§¨‡§®\b', '‡§¨‡§Ç‡§¶'),
            (r'\b‡§¨‡§®‡•ç‡§§\b', '‡§¨‡§Ç‡§¶'),
            (r'\b‡§¨‡§®‡•ç‡§§‡•á\b', '‡§¨‡§Ç‡§¶'),
            (r'\b‡§¨‡§Ç‡§¶‡•ç‡§§‡•á\b', '‡§¨‡§Ç‡§¶'),
            (r'\b‡§¨‡§®‡•ç‡§§‡•ã‡§ú‡§æ\b', '‡§¨‡§Ç‡§¶ ‡§ï‡§∞‡•ã'),
            (r'\b‡§¨‡§Ç‡§¶‡•ç‡§§‡•ã‡§ú‡§æ\b', '‡§¨‡§Ç‡§¶ ‡§ï‡§∞‡•ã'),
            (r'‡§¨‡§® ‡§ï‡§∞‡•ã', '‡§¨‡§Ç‡§¶ ‡§ï‡§∞‡•ã'),
            (r'‡§µ‡§® ‡§ï‡§∞‡•ã', '‡§¨‡§Ç‡§¶ ‡§ï‡§∞‡•ã'),
            (r'‡§¨‡§Ç‡§¶‡§ï‡§∞‡•ã', '‡§¨‡§Ç‡§¶ ‡§ï‡§∞‡•ã'),
            (r'‡§∏‡§Æ‡§Ø‡•á', '‡§∏‡§Æ‡§Ø'),
            (r'‡§¨‡§§‡§ì', '‡§¨‡§§‡§æ‡§ì'),
            (r'‡§ï‡•ç‡§Ø\b', '‡§ï‡•ç‡§Ø‡§æ'),
            (r'‡§ï‡§ø‡§§‡§®\b', '‡§ï‡§ø‡§§‡§®‡•á'),
            (r'‡§Æ‡•Å‡§ù\b', '‡§Æ‡•Å‡§ù‡•á'), (r'\b‡§Æ‡•Å‡§ú‡•á\b', '‡§Æ‡•Å‡§ù‡•á'),
            (r'‡§§‡§ø‡§•\b', '‡§§‡§ø‡§•‡§ø'),
            (r'‡§§‡§æ‡§∞‡§ø‡§ñ', '‡§§‡§æ‡§∞‡•Ä‡§ñ'),
            (r'‡§ï‡§∞‡§¶‡•ã', '‡§ï‡§∞ ‡§¶‡•ã'),
            (r'‡§π‡•ã‡§ú‡§æ‡§ì', '‡§π‡•ã ‡§ú‡§æ‡§ì'),
            (r'‡§ï‡•ã‡§®\b', '‡§ï‡•å‡§®'),
            (r'‡§ï‡§æ‡§â‡§®', '‡§ï‡•å‡§®'),
            (r'‡§®‡§Æ‡§∏‡§§‡•á', '‡§®‡§Æ‡§∏‡•ç‡§§‡•á'),
            (r'‡§®‡§Æ‡§∏‡•ç‡§§', '‡§®‡§Æ‡§∏‡•ç‡§§‡•á'),
            (r'‡§™‡§≤‡•Ä‡§ú', 'please'),
            (r'‡§π‡•á‡§≤‡•ç‡§™', 'help'),
            (r'‡§π‡•á‡§≤‡•ç‡§•', 'help'),
            (r'\bwat\b', 'what'),
            (r'\btym\b', 'time'),
            (r'\bplz\b', 'please'),
            (r'\bstap\b', 'stop'),
            
            # --- Heavy-Duty Romanized to Devanagari Bridge ---
            (r'\bsamay\b', '‡§∏‡§Æ‡§Ø'), (r'\bsamae\b', '‡§∏‡§Æ‡§Ø'), (r'\bsmae\b', '‡§∏‡§Æ‡§Ø'), (r'\bsama\b', '‡§∏‡§Æ‡§Ø'), (r'\bsame\b', '‡§∏‡§Æ‡§Ø'),
            (r'\bkya\b', '‡§ï‡•ç‡§Ø‡§æ'), (r'\bkiya\b', '‡§ï‡•ç‡§Ø‡§æ'), (r'\bkae\b', '‡§ï‡•ç‡§Ø‡§æ'),
            (r'\bhai\b', '‡§π‡•à'), (r'\bha\b', '‡§π‡•à'), (r'\bhura\b', '‡§π‡•ã ‡§∞‡§π‡§æ'), (r'\bho\b', '‡§π‡•ã'), (r'\bhai\b', '‡§π‡•à'),
            (r'\btariq\b', '‡§§‡§æ‡§∞‡•Ä‡§ñ'), (r'\btarikh\b', '‡§§‡§æ‡§∞‡•Ä‡§ñ'),
            (r'\bnamaste\b', '‡§®‡§Æ‡§∏‡•ç‡§§‡•á'), (r'\bnamasitai\b', '‡§®‡§Æ‡§∏‡•ç‡§§‡•á'),
            # THANK_YOU INTENT - Critical phonetic fixes
            (r'\bshukriya\b', '‡§∂‡•Å‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ'), (r'\bshukriyaa\b', '‡§∂‡•Å‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ'), (r'\bsukriya\b', '‡§∂‡•Å‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ'), (r'\bsukria\b', '‡§∂‡•Å‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ'),
            (r'\b‡§ú‡•Å‡§™‡•ç‡§∞‡§ø‡§Ø‡§æ\b', '‡§∂‡•Å‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ'), (r'\b‡§∏‡•Å‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ\b', '‡§∂‡•Å‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ'), (r'\b‡§∏‡•Å‡§™‡•ç‡§∞‡§ø‡§Ø‡§æ\b', '‡§∂‡•Å‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ'), (r'\b‡§ú‡•Å‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ\b', '‡§∂‡•Å‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ'),
            # JOKE INTENT - Critical phonetic fixes
            (r'\b‡§ú‡•Å‡§ï‡•ç‡§∞\b', '‡§ú‡•ã‡§ï'), (r'\b‡§ú‡•Å‡§ï‡•ç‡§∞‡§æ\b', '‡§ú‡•ã‡§ï'), (r'\b‡§ú‡•Å‡§ï\b', '‡§ú‡•ã‡§ï'), (r'\b‡§Æ‡§ú‡§æ‡§ï‡•ç‡§∞\b', '‡§Æ‡§ú‡§æ‡§ï'),
            # MUSIC INTENT - Phonetic consistency
            (r'\b‡§¨‡§Ç‡§¶‡§æ‡§ì\b', '‡§¨‡§ú‡§æ‡§ì'), (r'\b‡§¨‡§Ç‡§¶‡§æ‡§®‡§æ‡§ì\b', '‡§¨‡§ú‡§æ‡§ì'), (r'\b‡§™‡§¶‡§æ‡§ì\b', '‡§¨‡§ú‡§æ‡§ì'), (r'\b‡§ï‡§æ‡§®‡§æ\b', '‡§ó‡§æ‡§®‡§æ'),
            # NEWS INTENT - Phonetic consistency
            (r'\b‡§∏‡§Æ‡§ú‡§æ‡§∞\b', '‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞'), (r'\b‡§∏‡§Æ‡§ö‡§æ‡§∞\b', '‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞'), (r'\b‡§∏‡§Æ‡§æ‡§¶‡•ç‡§Ø‡§æ‡§∞\b', '‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞'),
            (r'\b‡§∏‡•Å‡§ï‡•ç‡§Ø‡§æ\b', '‡§∂‡•Å‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ'), (r'\b‡§¶‡§π‡§®‡§ø‡§µ‡§æ‡§¶\b', '‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶'), (r'\b‡§ß‡§®‡§ø‡§µ‡§æ‡§¶\b', '‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶'),
            (r'\baaj\b', '‡§Ü‡§ú'), (r'\baach\b', '‡§Ü‡§ú'), (r'\baj\b', '‡§Ü‡§ú'), (r'\bad\b', '‡§Ü‡§ú'),
            (r'\bmadad\b', '‡§Æ‡§¶‡§¶'), (r'\bmodot\b', '‡§Æ‡§¶‡§¶'), (r'\bmodat\b', '‡§Æ‡§¶‡§¶'),
            (r'\bvither\b', '‡§Æ‡•å‡§∏‡§Æ'), (r'\bweather\b', '‡§Æ‡•å‡§∏‡§Æ'), (r'\bwather\b', '‡§Æ‡•å‡§∏‡§Æ'), (r'\bmoasam\b', '‡§Æ‡•å‡§∏‡§Æ'), (r'\bwethar\b', '‡§Æ‡•å‡§∏‡§Æ'), (r'\bmosaam\b', '‡§Æ‡•å‡§∏‡§Æ'), (r'\bmonsam\b', '‡§Æ‡•å‡§∏‡§Æ'), (r'\bmousam\b', '‡§Æ‡•å‡§∏‡§Æ'),
            # JOKE INTENT - Critical phonetic fixes
            (r'\bjoke\b', '‡§ú‡•ã‡§ï'), (r'\bjok\b', '‡§ú‡•ã‡§ï'), (r'\b‡§ú‡•Å‡§ï\b', '‡§ú‡•ã‡§ï'),
            (r'\bmazaq\b', '‡§Æ‡§ú‡§æ‡§ï'), (r'\bmazak\b', '‡§Æ‡§ú‡§æ‡§ï'), (r'\b‡§Æ‡§ö‡§æ‡§ï\b', '‡§Æ‡§ú‡§æ‡§ï'), (r'\b‡§Æ‡§ú‡§ï\b', '‡§Æ‡§ú‡§æ‡§ï'),
            # MUSIC INTENT - Critical phonetic fixes
            (r'\bgana\b', '‡§ó‡§æ‡§®‡§æ'), (r'\bgaana\b', '‡§ó‡§æ‡§®‡§æ'), (r'\bsong\b', '‡§ó‡§æ‡§®‡§æ'), (r'\b‡§ï‡§æ‡§®‡§æ\b', '‡§ó‡§æ‡§®‡§æ'), (r'\b‡§ï‡§®‡§æ\b', '‡§ó‡§æ‡§®‡§æ'), (r'\b‡§ï‡§æ‡§®‡•ã\b', '‡§ó‡§æ‡§®‡§æ'), (r'\b‡§¨‡§æ‡§®‡§æ\b', '‡§ó‡§æ‡§®‡§æ'), (r'\b‡§ñ‡§æ‡§®‡§æ\b', '‡§ó‡§æ‡§®‡§æ'),
            # DANCE INTENT - Critical phonetic fixes
            (r'\bnaaj\b', '‡§®‡§æ‡§ö'), (r'\bnaach\b', '‡§®‡§æ‡§ö'), (r'\bdance\b', '‡§°‡§æ‡§Ç‡§∏'), (r'\bnaacu\b', '‡§®‡§æ‡§ö‡•ã'), (r'\bnaachu\b', '‡§®‡§æ‡§ö‡•ã'), (r'\bnachiye\b', '‡§®‡§æ‡§ö‡•ã'), (r'\b‡§∞‡§æ‡§ö\b', '‡§®‡§æ‡§ö'), (r'\b‡§®‡§ú\b', '‡§®‡§æ‡§ö'),
            (r'\bnathke\b', '‡§®‡§æ‡§ö‡§ï‡•á'), (r'\bnatchke\b', '‡§®‡§æ‡§ö‡§ï‡•á'), (r'\bnath\b', '‡§®‡§æ‡§ö'), (r'\bnatch\b', '‡§®‡§æ‡§ö'),
            (r'\balvida\b', '‡§Ö‡§≤‡§µ‡§ø‡§¶‡§æ'), (r'\bbye\b', 'bye'),
            (r'\bdhanyawad\b', '‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶'), (r'\bdhanyavad\b', '‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶'), (r'\bdhani\b', '‡§ß‡§®‡•ç‡§Ø'), (r'\bavad\b', '‡§µ‡§æ‡§¶'), (r'\bdanny\b', '‡§ß‡§®‡•ç‡§Ø'),
            (r'\bbatau\b', '‡§¨‡§§‡§æ‡§ì'), (r'\bbata\b', '‡§¨‡§§‡§æ‡§ì'), (r'\bbatai\b', '‡§¨‡§§‡§æ‡§ì'), (r'\bbatah\b', '‡§¨‡§§‡§æ‡§ì'), (r'\bbato\b', '‡§¨‡§§‡§æ‡§ì'),
            (r'\baapka\b', '‡§Ü‡§™‡§ï‡§æ'), (r'\baap\b', '‡§Ü‡§™'), (r'\bmyri\b', '‡§Æ‡•á‡§∞‡•Ä'), (r'\bmerili\b', '‡§Æ‡•á‡§∞‡•á ‡§≤‡§ø‡§è'), (r'\bleah\b', '‡§≤‡§ø‡§è'), (r'\blea\b', '‡§≤‡§ø‡§è'),
            (r'\bbhaaut\b', '‡§¨‡§π‡•Å‡§§'), (r'\bbhaut\b', '‡§¨‡§π‡•Å‡§§'), (r'\btoda\b', '‡§•‡•ã‡§°‡§º‡§æ'), (r'\btora\b', '‡§•‡•ã‡§°‡§º‡§æ'),
            (r'\bband\b', '‡§¨‡§Ç‡§¶'), (r'\bbanth\b', '‡§¨‡§Ç‡§¶'), (r'\bbandh\b', '‡§¨‡§Ç‡§¶'), (r'\bkaro\b', '‡§ï‡§∞‡•ã'), (r'\bkaru\b', '‡§ï‡§∞‡•ã'),
            (r'\bkesar\b', '‡§ï‡•à‡§∏‡§æ'), (r'\bkaisa\b', '‡§ï‡•à‡§∏‡§æ'),
            (r'\bdhikh\b', '‡§¶‡§ø‡§ñ'), (r'\bdikh\b', '‡§¶‡§ø‡§ñ'), (r'\bkao\b', '‡§¶‡§ø‡§ñ‡§æ‡§ì'), (r'\bdhikao\b', '‡§¶‡§ø‡§ñ‡§æ‡§ì'), (r'\bdikao\b', '‡§¶‡§ø‡§ñ‡§æ‡§ì'), (r'\bkautura\b', '‡§¶‡§ø‡§ñ‡§æ‡§ì'), (r'\bkarwek\b', '‡§ï‡§∞‡§ï‡•á'),
            (r'\bwaqt\b', '‡§µ‡§ï‡•ç‡§§'), (r'\bwakt\b', '‡§µ‡§ï‡•ç‡§§'),
            (r'samayakya', '‡§∏‡§Æ‡§Ø ‡§ï‡•ç‡§Ø‡§æ'), (r'samaybatau', '‡§∏‡§Æ‡§Ø ‡§¨‡§§‡§æ‡§ì'), (r'samaykyahai', '‡§∏‡§Æ‡§Ø ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à'),
            (r'samaykyahora', '‡§∏‡§Æ‡§Ø ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•ã ‡§∞‡§π‡§æ'), (r'samayhora', '‡§∏‡§Æ‡§Ø ‡§π‡•ã ‡§∞‡§π‡§æ'),
            (r'bandhojao', '‡§¨‡§Ç‡§¶ ‡§ï‡§∞‡•ã'), (r'mandojao', '‡§¨‡§Ç‡§¶ ‡§ï‡§∞‡•ã'), (r'bantujao', '‡§¨‡§Ç‡§¶ ‡§ï‡§∞‡•ã'), (r'bandho', '‡§¨‡§Ç‡§¶ ‡§ï‡§∞‡•ã'),
            (r'\bguit\b', 'quit'), (r'\bshuit\b', 'quit'), (r'\bquit\b', 'quit'), (r'\bexit\b', 'exit'),
            (r"today's mosam", '‡§Ü‡§ú ‡§ï‡§æ ‡§Æ‡•å‡§∏‡§Æ'), (r'what will happen', 'weather ‡§¨‡§§‡§æ‡§ì'), (r'how will it live', '‡§ï‡•à‡§∏‡§æ ‡§∞‡§π‡•á‡§ó‡§æ'),
            (r'banthkaru', '‡§¨‡§Ç‡§¶ ‡§ï‡§∞‡•ã'), (r'banthkaro', '‡§¨‡§Ç‡§¶ ‡§ï‡§∞‡•ã'), (r'sukriya', '‡§∂‡•Å‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ'), (r'sukria', '‡§∂‡•Å‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ'),
            (r'\bsocial\b', '‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞'), (r'\bsociety\b', '‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞'), (r'\bsamachar\b', '‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞'), (r'\bsamacar\b', '‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞'),
            (r'\btopic\b', '‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞'), (r'\bknife\b', '‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞'), (r'\buse\b', '‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞'), (r'\blet us know\b', '‡§¨‡§§‡§æ‡§ì'),
            (r'\bsama\s*chhar\b', '‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞'), (r'\bsamachhar\b', '‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞'), (r'\bsamachar\b', '‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞'),
            (r'\bchhar\b', '‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞'), (r'\bchahar\b', '‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞'), (r'\bchar\b', '‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞'),
            (r'\bnews\b', '‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞'), (r'\bnuse\b', '‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞'), (r'\bnuze\b', '‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞'),
            (r'\bbantuja\b', '‡§¨‡§Ç‡§¶ ‡§ï‡§∞‡•ã'), (r'\bbantuja\s*ho\b', '‡§¨‡§Ç‡§¶ ‡§ï‡§∞‡•ã'), (r'\bbanthoja\b', '‡§¨‡§Ç‡§¶ ‡§ï‡§∞‡•ã'),
            (r'\bbantujao\b', '‡§¨‡§Ç‡§¶ ‡§ï‡§∞‡•ã'), (r'\bbandoja\b', '‡§¨‡§Ç‡§¶ ‡§ï‡§∞‡•ã'), (r'\bbanthuja\b', '‡§¨‡§Ç‡§¶ ‡§ï‡§∞‡•ã'),
            (r'‡§Ö‡§ú ‡§ï‡•á ‡§¶‡§ø‡§ï‡§æ‡§∞', '‡§®‡§æ‡§ö ‡§ï‡•á ‡§¶‡§ø‡§ñ‡§æ‡§ì'), (r'‡§Ö‡§ú ‡§ï‡•á ‡§§‡§ø‡§ï‡§æ‡§ì', '‡§®‡§æ‡§ö ‡§ï‡•á ‡§¶‡§ø‡§ñ‡§æ‡§ì'), (r'‡§Ö‡§ú ‡§ï‡•á', '‡§π‡§Æ‡•á‡§Ç'),
            (r'dhannewad', '‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶‡•ç'), (r'dhanewad', '‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶‡•ç'),
            
            # --- Transliterated Urdu Fragment Bridge ---
            (r'‡§¶‡§π‡§®‡§Ø', '‡§ß‡§®‡•ç‡§Ø'), (r'‡§Ü‡§µ‡§Ö‡§¶', '‡§µ‡§æ‡§¶'), (r'‡§Ö‡§µ‡§æ‡§Ö‡§¶', '‡§µ‡§æ‡§¶'), (r'‡§¶‡§®‡§ó', '‡§ß‡§®‡•ç‡§Ø'), (r'‡§¶‡§π‡§®‡§ó‡§Ø', '‡§ß‡§®‡•ç‡§Ø'), (r'‡§µ‡§Ö‡§ú', '‡§µ‡§æ‡§¶'), (r'‡§ï‡§Ö', '‡§ï‡§æ'), (r'‡§Æ‡§¶‡§¶', '‡§Æ‡§¶‡§¶'),
            
            # --- Urdu Script Bridge (Unicode Hallucination Fix) ---
            (r"\u0622\u0686", "‡§Ü‡§ú"), (r"\u0633\u0645", "‡§∏‡§Æ"), (r"\u0686\u0627\u0631", "‡§ö‡§æ‡§∞"), (r"\u062c\u0648\u06a9", "‡§ú‡•ã‡§ï"), (r"\u0645\u0630\u0627\u06a9", "‡§Æ‡§ú‡§æ‡§ï"),
            (r"\u06a9\u06cc\u0627", "‡§ï‡•ç‡§Ø‡§æ"), (r"\u06c1\u06d2", "‡§π‡•à"), (r"\u0628\u062a\u0620", "‡§¨‡§§‡§æ‡§ì"),
            
            # --- Phonetic Devanagari Corrections ---
            (r'\b‡§Æ‡§∏‡§Æ\b', '‡§Æ‡•å‡§∏‡§Æ'), (r'\b‡§Æ‡•ã‡§∏‡§Æ\b', '‡§Æ‡•å‡§∏‡§Æ'),
            (r'\b‡§¨‡§∞‡§∂\b', '‡§¨‡§æ‡§∞‡§ø‡§∂'), (r'\b‡§†‡§°\b', '‡§†‡§Ç‡§°'),
            (r'\b‡§ó‡§∞‡§Æ\b', '‡§ó‡§∞‡•ç‡§Æ‡•Ä'), (r'\b‡§ú‡§ï\b', '‡§ú‡•ã‡§ï'),
            (r'\‡§¨‡§Æ‡§ú‡§ï\b', '‡§Æ‡§ú‡§æ‡§ï'), (r'\b‡§ö‡§ü‡§ï‡§≤\b', '‡§ö‡•Å‡§ü‡§ï‡•Å‡§≤‡§æ'),
            (r'\b‡§ó‡§®\b', '‡§ó‡§æ‡§®‡§æ'), (r'\b‡§∏‡§ó‡§§\b', '‡§∏‡§Ç‡§ó‡•Ä‡§§'),
            (r'\b‡§Ö‡§≤‡§∞‡§Æ\b', '‡§Ö‡§≤‡§æ‡§∞‡•ç‡§Æ'), (r'\b‡§∞‡§Æ‡§á‡§°‡§∞\b', '‡§∞‡§ø‡§Æ‡§æ‡§á‡§Ç‡§°‡§∞'),
            (r'\b‡§∏‡§Æ‡§ö‡§∞\b', '‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞'), (r'\b‡§®‡§Ø‡§ú‡§º\b', '‡§®‡•ç‡§Ø‡•Ç‡§ú‡§º'),
            (r'\b‡§ñ‡§¨‡§∞\b', '‡§ñ‡§¨‡§∞'),
            
            # Music intent variants (‡§ó‡§æ‡§®‡§æ)
            (r'\bganna\b', '‡§ó‡§æ‡§®‡§æ'), (r'\bgana\b', '‡§ó‡§æ‡§®‡§æ'), (r'\bkanna\b', '‡§ó‡§æ‡§®‡§æ'),
            (r'\bkana\b', '‡§ó‡§æ‡§®‡§æ'), (r'\bganaa\b', '‡§ó‡§æ‡§®‡§æ'),
            (r'\bmujhe\s+ganna\b', '‡§ó‡§æ‡§®‡§æ'), (r'\bmujee\s+kanna\b', '‡§ó‡§æ‡§®‡§æ'),
            (r'\bsunao\b', '‡§∏‡•Å‡§®‡§æ‡§ì'), (r'\bsuna\b', '‡§∏‡•Å‡§®‡§æ‡§ì'), (r'\bsunaai\b', '‡§∏‡•Å‡§®‡§æ‡§ì'), (r'\b‡§∏‡•Å‡§≤‡§æ\b', '‡§∏‡•Å‡§®‡§æ‡§ì'),
            
            # Weather intent variants (‡§Æ‡•å‡§∏‡§Æ)
            (r'\bviter\b', '‡§Æ‡•å‡§∏‡§Æ'), (r'\bwither\b', '‡§Æ‡•å‡§∏‡§Æ'), (r'\bvether\b', '‡§Æ‡•å‡§∏‡§Æ'),
            (r'\bviter\s+batal\b', '‡§Æ‡•å‡§∏‡§Æ ‡§¨‡§§‡§æ‡§ì'),
            (r'\bbatal\b', '‡§¨‡§§‡§æ‡§ì'), (r'\bbata\b', '‡§¨‡§§‡§æ‡§ì'),
        ]
        
        # Heavy-Duty Perso-Arabic (Urdu) to Devanagari character mapping
        self.urdu_map = {
            '\u0622': '‡§Ü', '\u0627': '‡§Ö', '\u0628': '‡§¨', '\u067e': '‡§™', '\u062a': '‡§§', 
            '\u0672': '‡§ü', '\u062b': '‡§∏', '\u062c': '‡§ú', '\u0686': '‡§ö', '\u062d': '‡§π', 
            '\u062e': '‡§ñ', '\u062f': '‡§¶', '\u0688': '‡§°', '\u0630': '‡§ú', '\u0631': '‡§∞', 
            '\u0632': '‡§ú', '\u0698': '‡§ù', '\u0633': '‡§∏', '\u0634': '‡§∂', '\u0635': '‡§∏', 
            '\u0636': '‡§ú', '\u0637': '‡§§', '\u0638': '‡§ú', '\u0639': '‡§Ö', '\u063a': '‡§ó', 
            '\u0641': '‡§´', '\u0642': '‡§ï', '\u06a9': '‡§ï', '\u06af': '‡§ó', '\u0644': '‡§≤', 
            '\u0645': '‡§Æ', '\u0646': '‡§®', '\u06ba': '‡§®', '\u0648': '‡§µ', '\u06c1': '‡§π', 
            '\u06be': '‡§π', '\u06d2': '‡§è', '\u06a4': '‡§µ', '\u06cc': '‡§Ø', '\u064a': '‡§Ø',
            '\u0626': '‡§è', '\u064b': '‡§®', '\u0621': '‡§á', '\u0624': '‡§ì'
        }
        
        try:
            from rapidfuzz import fuzz
            self.fuzz = fuzz
            self.use_fuzzy = True
            self.fuzzy_threshold = 80
        except ImportError:
            self.use_fuzzy = False

    def _transliterate_perso_arabic_to_devanagari(self, text):
        """Character-level conversion of Urdu script to Devanagari"""
        result = []
        for char in text:
            if '\u0600' <= char <= '\u06FF':
                result.append(self.urdu_map.get(char, ''))
            else:
                result.append(char)
        return "".join(result)

    def correct(self, text):
        if not text: return ""
        original_text = text
        
        # Pass 0: Aggressive Urdu-to-Hindi character transliteration
        text = self._transliterate_perso_arabic_to_devanagari(text)
        
        # Pass 0.5: Normalize spaces (fixes "Sama Chhar" ‚Üí "samachhar")
        text = re.sub(r'\s+', ' ', text)  # Multiple spaces ‚Üí single space
        text = text.strip()
        
        # Pass 0.75: Noise cleanup
        noise_words = r'\b(umm|uh|hmm|aah|uhh|like|you know|bhujhey|mujee|aa|eh)\b'
        text = re.sub(noise_words, '', text, flags=re.IGNORECASE)
        text = re.sub(r'([a-zA-Z])\1{2,}', r'\1\1', text)  # "Gannna" ‚Üí "Ganna"
        text = re.sub(r'\s+', ' ', text).strip()
        
        # Pass 1: Regex patterns (Case-insensitive for Romanized parts)
        corrected = text
        for pattern, replacement in self.error_patterns:
            corrected = re.sub(pattern, replacement, corrected, flags=re.IGNORECASE)
        
        # Pass 2: Word-level fuzzy correction
        words = corrected.split()
        corrected_words = []
        for word in words:
            corrected_word = self._correct_word(word)
            corrected_words.append(corrected_word)
        
        final_text = ' '.join(corrected_words)
        if final_text != original_text:
            print(f"‚úèÔ∏è  Corrected: '{original_text}' ‚Üí '{final_text}'")
        return final_text
    
    def _correct_word(self, word):
        if not self.use_fuzzy or len(word) < 2: return word
        word_lower = word.lower()
        
        # Pass 1: Check for exact match first (Avoid over-correction like naacho -> naach)
        for category, vocab_list in self.core_vocabulary.items():
            for vocab_word in vocab_list:
                if word_lower == vocab_word.lower():
                    return word
        
        # Pass 2: Fuzzy matching only if no exact match found
        best_match = word
        best_score = 0
        
        for category, vocab_list in self.core_vocabulary.items():
            for vocab_word in vocab_list:
                similarity = self.fuzz.ratio(word_lower, vocab_word.lower())
                if similarity > best_score:
                    best_score = similarity
                    best_match = vocab_word
        
        if best_score >= self.fuzzy_threshold:
            return best_match
            
        return word

# ============================================================
# LAYER 3: ROBUST INTENT CLASSIFICATION
# ============================================================

class RobustIntentClassifier:
    def __init__(self, model_path=None, use_onnx=True):
        """
        Initialize intent classifier with ONNX optimization
        Falls back to PyTorch if ONNX model not found
        """
        if model_path is None:
            script_dir = os.path.dirname(os.path.abspath(__file__))
            model_path = os.path.join(script_dir, 'hindi_intent_model_final')
        
        # Check for ONNX model
        onnx_path = model_path.replace('_final', '_onnx_int8')
        
        if use_onnx and os.path.exists(onnx_path):
            print(f"‚öôÔ∏è  Loading ONNX-optimized classifier from {os.path.basename(onnx_path)}...")
            try:
                self._load_onnx_model(onnx_path)
                return  # Success, skip PyTorch loading
            except Exception as e:
                print(f"‚ö†Ô∏è  ONNX loading failed: {e}")
                print(f"   Falling back to PyTorch model...")
        
        # Load PyTorch model (original or fallback)
        if use_onnx and not os.path.exists(onnx_path):
            print(f"‚ÑπÔ∏è  ONNX model not found at {os.path.basename(onnx_path)}")
            print(f"   Using PyTorch model (run convert_indicbert_to_onnx.py to optimize)")
        
        print(f"‚öôÔ∏è  Loading PyTorch classifier from {os.path.basename(model_path)}...")
        self._load_pytorch_model(model_path)

    def _load_onnx_model(self, model_path):
        """Load ONNX-optimized model"""
        import os
        import json
        import torch
        from optimum.onnxruntime import ORTModelForSequenceClassification
        
        # Load label map
        with open(os.path.join(model_path, 'label_map.json'), 'r') as f:
            self.id2label = json.load(f)['id2label']
        
        # Load tokenizer and model
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = ORTModelForSequenceClassification.from_pretrained(
            model_path,
            provider="CPUExecutionProvider"
        )
        
        self.device = torch.device("cpu")
        self.model_type = "onnx"
        
        print("   ‚úì ONNX INT8 model loaded")
        
        # Pin to A76 cores (Cubie A7A optimization)
        try:
            import psutil
            p = psutil.Process()
            p.cpu_affinity([0, 1])  # Cores 0-1 are Cortex-A76
            print("   ‚úì Process pinned to Cortex-A76 cores")
        except Exception:
            pass  # Not critical
        
        # Set thread limits for 6GB RAM
        os.environ['OMP_NUM_THREADS'] = '2'
        torch.set_num_threads(2)
        
        # Robust fallback keywords for 13 intents
        self.fallback_patterns = {
            'stop': ['‡§¨‡§Ç‡§¶', '‡§∏‡•ç‡§ü‡•â‡§™', 'stop', '‡§∞‡•Å‡§ï‡•ã', '‡§∞‡•Ç‡§ï‡•ã', 'exit', 'quit', 'close', '‡§¨‡§®‡•ç‡§¶', '‡§∏‡§Æ‡§æ‡§™‡•ç‡§§', '‡§ñ‡§§‡•ç‡§Æ', 'band', 'bantuja', '‡§Ö‡§≤‡§µ‡§ø‡§¶‡§æ', '‡§Ö‡§≤‡§µ‡•Ä‡§¶‡§æ', '‡§¨‡§æ‡§Ø', 'bye', '‡§ü‡§æ‡§ü‡§æ', '‡§ó‡•Å‡§°‡§¨‡§æ‡§Ø', 'alvida'],
            'time': ['‡§∏‡§Æ‡§Ø', '‡§ü‡§æ‡§á‡§Æ', 'time', '‡§¨‡§ú‡•á', '‡§ò‡§°‡§º‡•Ä', '‡§µ‡§ï‡•ç‡§§', '‡§ò‡§Ç‡§ü‡§æ', '‡§ò‡§Ç‡§ü‡•á', 'samay', 'samai', 'time', 'samaya'],
            'date': ['‡§§‡§æ‡§∞‡•Ä‡§ñ', '‡§§‡§ø‡§•‡§ø', '‡§°‡•á‡§ü', 'date', '‡§Ü‡§ú', '‡§¶‡§ø‡§®', '‡§ï‡•à‡§≤‡•á‡§Ç‡§°‡§∞', 'tariq', 'tarikh', 'tithi', 'din'],
            'hello': ['‡§®‡§Æ‡§∏‡•ç‡§§‡•á', '‡§®‡§Æ‡§∏‡•ç‡§ï‡§æ‡§∞', '‡§π‡•à‡§≤‡•ã', '‡§π‡•á‡§≤‡•ã', 'hello', 'hi', '‡§π‡§æ‡§Ø', '‡§™‡•ç‡§∞‡§£‡§æ‡§Æ', 'namaste', 'naam', 'name', '‡§®‡§æ‡§Æ'],
            'thank_you': ['‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶', '‡§∂‡•Å‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ', 'thanks', 'thank', '‡§•‡•à‡§Ç‡§ï', '‡§Ü‡§≠‡§æ‡§∞', '‡§∂‡•Å‡§ï‡•ç‡§∞‡•Ä‡§Ø‡§æ', 'shukriya', '‡§ú‡•Å‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ', '‡§ú‡•Å‡§™‡•ç‡§∞‡§ø‡§Ø‡§æ'],
            'help': ['‡§Æ‡§¶‡§¶', '‡§π‡•á‡§≤‡•ç‡§™', 'help', '‡§∏‡§π‡§æ‡§Ø‡§§‡§æ', '‡§∏‡§π‡§æ‡§Ø‡§§', 'madad'],
            'dance': ['‡§®‡§æ‡§ö', 'dance', '‡§®‡§æ‡§ö‡•ã', '‡§°‡§æ‡§Ç‡§∏', '‡§¶‡§ø‡§ï‡§æ‡§∞', '‡§§‡§ø‡§ï‡§æ‡§ì', '‡§¶‡§ø‡§ñ‡§æ‡§ì'],
            'weather': ['‡§Æ‡•å‡§∏‡§Æ', 'weather', '‡§¨‡§æ‡§∞‡§ø‡§∂' ,'‡§†‡§Ç‡§°', '‡§ó‡§∞‡•ç‡§Æ‡•Ä', '‡§§‡§æ‡§™‡§Æ‡§æ‡§®', 'viter', 'wither', 'vether', 'batal'],
            'joke': ['‡§ú‡•ã‡§ï', 'joke', '‡§Æ‡§ú‡§æ‡§ï', '‡§π‡§Å‡§∏‡§æ‡§ì', 'funny', '‡§ö‡•Å‡§ü‡§ï‡•Å‡§≤‡§æ', '‡§ï‡•â‡§Æ‡•á‡§°‡•Ä', '‡§ú‡•Å‡§ï‡•ç‡§∞', '‡§ú‡•Å‡§ï‡•ç‡§∞‡§æ'],
            'music': ['‡§ó‡§æ‡§®‡§æ', '‡§∏‡§Ç‡§ó‡•Ä‡§§', 'music', 'song', '‡§¨‡§ú‡§æ‡§ì', '‡§ö‡§≤‡§æ‡§ì', 'play', 'ganna', 'gana', 'kanna', 'kana', 'sunao', 'suna', '‡§¨‡§Ç‡§¶‡§æ‡§®‡§æ‡§ì', '‡§¨‡§Ç‡§¶‡§æ‡§®‡§æ', '‡§¨‡§ú‡§æ', 'bajao', '‡§¨‡§Ç‡§¶‡§æ‡§ì'],
            'news': ['‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞', '‡§®‡•ç‡§Ø‡•Ç‡§ú‡§º', 'news', ' ‡§ñ‡§¨‡§∞', 'headlines', '‡§Ö‡§™‡§°‡•á‡§ü', 'chhar', 'char', '‡§ö‡§æ‡§∞', '‡§ö‡§∞', 'samachhar', '‡§∏‡§Æ‡§ú‡§æ‡§∞', '‡§∏‡§Æ‡§æ‡§¶‡•ç‡§Ø‡§æ‡§∞'],
        }

    def _load_pytorch_model(self, model_path):
        """Load original PyTorch model (fallback)"""
        with open(os.path.join(model_path, 'label_map.json'), 'r') as f:
            self.id2label = json.load(f)['id2label']
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            model_path
        )
        self.model.eval()
        self.device = torch.device("cpu")
        self.model.to(self.device)
        self.model_type = "pytorch"
        
        print("   ‚úì PyTorch float32 model loaded")
        
        # Keep fallback patterns (add them here too)
        self.fallback_patterns = {
            'stop': ['‡§¨‡§Ç‡§¶', '‡§∏‡•ç‡§ü‡•â‡§™', 'stop', '‡§∞‡•Å‡§ï‡•ã', '‡§∞‡•Ç‡§ï‡•ã', 'exit', 'quit', 'close', '‡§¨‡§®‡•ç‡§¶', '‡§∏‡§Æ‡§æ‡§™‡•ç‡§§', '‡§ñ‡§§‡•ç‡§Æ', 'band', 'bantuja', '‡§Ö‡§≤‡§µ‡§ø‡§¶‡§æ', '‡§Ö‡§≤‡§µ‡•Ä‡§¶‡§æ', '‡§¨‡§æ‡§Ø', 'bye', '‡§ü‡§æ‡§ü‡§æ', '‡§ó‡•Å‡§°‡§¨‡§æ‡§Ø', 'alvida'],
            'time': ['‡§∏‡§Æ‡§Ø', '‡§ü‡§æ‡§á‡§Æ', 'time', '‡§¨‡§ú‡•á', '‡§ò‡§°‡§º‡•Ä', '‡§µ‡§ï‡•ç‡§§', '‡§ò‡§Ç‡§ü‡§æ', '‡§ò‡§Ç‡§ü‡•á', 'samay', 'samai', 'time', 'samaya'],
            'date': ['‡§§‡§æ‡§∞‡•Ä‡§ñ', '‡§§‡§ø‡§•‡§ø', '‡§°‡•á‡§ü', 'date', '‡§Ü‡§ú', '‡§¶‡§ø‡§®', '‡§ï‡•à‡§≤‡•á‡§Ç‡§°‡§∞', 'tariq', 'tarikh', 'tithi', 'din'],
            'hello': ['‡§®‡§Æ‡§∏‡•ç‡§§‡•á', '‡§®‡§Æ‡§∏‡•ç‡§ï‡§æ‡§∞', '‡§π‡•à‡§≤‡•ã', '‡§π‡•á‡§≤‡•ã', 'hello', 'hi', '‡§π‡§æ‡§Ø', '‡§™‡•ç‡§∞‡§£‡§æ‡§Æ', 'namaste', 'naam', 'name', '‡§®‡§æ‡§Æ'],
            'thank_you': ['‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶', '‡§∂‡•Å‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ', 'thanks', 'thank', '‡§•‡•à‡§Ç‡§ï', '‡§Ü‡§≠‡§æ‡§∞', '‡§∂‡•Å‡§ï‡•ç‡§∞‡•Ä‡§Ø‡§æ', 'shukriya', '‡§ú‡•Å‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ', '‡§ú‡•Å‡§™‡•ç‡§∞‡§ø‡§Ø‡§æ'],
            'help': ['‡§Æ‡§¶‡§¶', '‡§π‡•á‡§≤‡•ç‡§™', 'help', '‡§∏‡§π‡§æ‡§Ø‡§§‡§æ', '‡§∏‡§π‡§æ‡§Ø‡§§', 'madad'],
            'dance': ['‡§®‡§æ‡§ö', 'dance', '‡§®‡§æ‡§ö‡•ã', '‡§°‡§æ‡§Ç‡§∏', '‡§¶‡§ø‡§ï‡§æ‡§∞', '‡§§‡§ø‡§ï‡§æ‡§ì', '‡§¶‡§ø‡§ñ‡§æ‡§ì'],
            'weather': ['‡§Æ‡•å‡§∏‡§Æ', 'weather', '‡§¨‡§æ‡§∞‡§ø‡§∂' ,'‡§†‡§Ç‡§°', '‡§ó‡§∞‡•ç‡§Æ‡•Ä', '‡§§‡§æ‡§™‡§Æ‡§æ‡§®', 'viter', 'wither', 'vether', 'batal'],
            'joke': ['‡§ú‡•ã‡§ï', 'joke', '‡§Æ‡§ú‡§æ‡§ï', '‡§π‡§Å‡§∏‡§æ‡§ì', 'funny', '‡§ö‡•Å‡§ü‡§ï‡•Å‡§≤‡§æ', '‡§ï‡•â‡§Æ‡•á‡§°‡•Ä', '‡§ú‡•Å‡§ï‡•ç‡§∞', '‡§ú‡•Å‡§ï‡•ç‡§∞‡§æ'],
            'music': ['‡§ó‡§æ‡§®‡§æ', '‡§∏‡§Ç‡§ó‡•Ä‡§§', 'music', 'song', '‡§¨‡§ú‡§æ‡§ì', '‡§ö‡§≤‡§æ‡§ì', 'play', 'ganna', 'gana', 'kanna', 'kana', 'sunao', 'suna', '‡§¨‡§Ç‡§¶‡§æ‡§®‡§æ‡§ì', '‡§¨‡§Ç‡§¶‡§æ‡§®‡§æ', '‡§¨‡§ú‡§æ', 'bajao', '‡§¨‡§Ç‡§¶‡§æ‡§ì', '‡§ï‡§æ‡§®‡§æ', '‡§™‡§¶‡§æ‡§ì'],
            'news': ['‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞', '‡§®‡•ç‡§Ø‡•Ç‡§ú‡§º', 'news', ' ‡§ñ‡§¨‡§∞', 'headlines', '‡§Ö‡§™‡§°‡•á‡§ü', 'chhar', 'char', '‡§ö‡§æ‡§∞', '‡§ö‡§∞', 'samachhar', '‡§∏‡§Æ‡§ú‡§æ‡§∞', '‡§∏‡§Æ‡§æ‡§¶‡•ç‡§Ø‡§æ‡§∞'],
        }

    def classify(self, text):
        if not text.strip(): return "unknown", 0.0
        
        # Robust Pre-processing (Strip punctuation, Urdu script residue, and Noise)
        text = re.sub(r'[.,!?‡•§|]', '', text).strip()
        # Strip remaining Urdu/Arabic characters if any leaked
        text = re.sub(r'[\u0600-\u06FF]', '', text).strip()
        text = re.sub(r'(?i)\b(teeke|theke|thek|tik|ok|hlo|hey)\b', '', text).strip()
        
        # Stage 0: Keyword Guardrails (Hard override for absolute clarity)
        words = set(text.lower().split())
        if any(w in words for w in ['‡§¶‡§ø‡§®', '‡§§‡§æ‡§∞‡•Ä‡§ñ', '‡§§‡§ø‡§•‡§ø', 'date', '‡§§‡§æ‡§∞‡•Ä‡§ï']):
            return "date", 0.99
        if any(w in words for w in ['‡§¨‡§ú‡§æ‡§ì', '‡§¨‡§Ç‡§¶‡§æ‡§®‡§æ‡§ì', '‡§¨‡§Ç‡§¶‡§æ‡§®‡§æ', '‡§ó‡§æ‡§®‡§æ', '‡§∏‡§Ç‡§ó‡•Ä‡§§', 'music', 'song', '‡§¨‡§ú‡§æ', '‡§¨‡§Ç‡§¶‡§æ‡§ì', '‡§ï‡§æ‡§®‡§æ', '‡§™‡§¶‡§æ‡§ì']):
            return "music", 0.99
        if any(w in words for w in ['‡§ú‡•ã‡§ï', 'joke', '‡§Æ‡§ú‡§æ‡§ï', '‡§ö‡•Å‡§ü‡§ï‡•Å‡§≤‡§æ', '‡§ú‡•Å‡§ï‡•ç‡§∞', '‡§ú‡•Å‡§ï‡•ç‡§∞‡§æ']):
            return "joke", 0.99
        if any(w in words for w in ['‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶', '‡§∂‡•Å‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ', 'thx', 'thanks', '‡§ú‡•Å‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ']):
            return "thank_you", 0.99
        if any(w in words for w in ['‡§∏‡§Æ‡§æ‡§¶‡•ç‡§Ø‡§æ‡§∞', '‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞', 'news', '‡§ñ‡§¨‡§∞', '‡§®‡•ç‡§Ø‡•Ç‡§ú‡§º', '‡§∏‡§Æ‡§ú‡§æ‡§∞']):
            return "news", 0.99
        if any(w in words for w in ['‡§®‡§æ‡§ö', '‡§®‡§æ‡§ö‡•ã', '‡§°‡§æ‡§Ç‡§∏', '‡§¶‡§ø‡§ï‡§æ‡§∞', '‡§§‡§ø‡§ï‡§æ‡§ì']):
            return "dance", 0.99
        
        # Stage 1: IndicBERT
        inputs = self.tokenizer(text, return_tensors="pt", max_length=64, truncation=True, padding=True).to(self.device)
        with torch.no_grad():
            outputs = self.model(**inputs)
            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
            conf, idx = torch.max(probs, dim=-1)
            
        intent = self.id2label.get(str(idx.item()), "unknown")
        confidence = conf.item()
        
        if confidence >= 0.82:
            # Stage 2: Stop Intent Sanity Check (Prevention of accidental exits)
            if intent == "stop":
                text_lower = text.lower()
                words = set(text_lower.split())
                # Must contain a stop keyword OR have extreme confidence
                has_stop_word = any(kw.lower() in words for kw in self.fallback_patterns['stop'])
                # Also check for substring match for compound Hindi phrases
                if not has_stop_word:
                    has_stop_word = any(kw.lower() in text_lower for kw in ['‡§¨‡§Ç‡§¶', '‡§∞‡•Å‡§ï‡•ã', 'stop', 'exit'])
                
                if not has_stop_word and confidence < 0.97:
                    print(f"‚ö†Ô∏è  Stop intent rejected (no keyword match). Conf: {confidence:.2f}")
                    return "unknown", confidence
            
            return intent, confidence
            
        # Try fuzzy fallback for EVERYTHING else
        fallback_intent = self._fuzzy_fallback(text)
        if fallback_intent:
            print(f"‚úì Fuzzy fallback matched: {fallback_intent}")
            return fallback_intent, 0.90
            
        return "unknown", confidence

    def _fuzzy_fallback(self, text):
        from rapidfuzz import fuzz
        text_lower = text.lower()
        
        # Pass 1: Local token-based presence (Strict)
        words = set(text_lower.split())
        for intent, keywords in self.fallback_patterns.items():
            for kw in keywords:
                if kw.lower() in words:
                    return intent
                    
        # Pass 2: Fuzzy Set Ratio (with safety checks)
        scores = {}
        for intent, keywords in self.fallback_patterns.items():
            max_score = 0
            for kw in keywords:
                # Ignore very short keywords for fuzzy matching to avoid "hi" in "abhi"
                if len(kw) < 3: continue 
                
                score = fuzz.token_set_ratio(text_lower, kw.lower())
                max_score = max(max_score, score)
            scores[intent] = max_score
            
        if scores:
            best_intent = max(scores, key=scores.get)
            if scores[best_intent] >= 80:
                return best_intent
            
        return None


# ============================================================
# MAIN ASSISTANT CLASS
# ============================================================

class RealtimeVoiceAssistant:
    def _check_memory_safety(self):
        """Ensure sufficient RAM for 6GB system"""
        try:
            import psutil
            mem = psutil.virtual_memory()
            
            free_gb = mem.available / (1024**3)
            total_gb = mem.total / (1024**3)
            
            print(f"üíæ Memory: {free_gb:.1f}GB free / {total_gb:.1f}GB total")
            
            if mem.available < 2.5 * 1024**3:  # Less than 2.5GB free
                print("‚ö†Ô∏è  WARNING: Low memory!")
                print(f"   Available: {free_gb:.1f}GB")
                print(f"   Recommended: 2.5GB minimum")
                print("   Close other applications for best performance.")
            
        except ImportError:
            print("‚ö†Ô∏è  psutil not installed (pip install psutil)")

    def __init__(self):
        # Memory safety check
        self._check_memory_safety()
        
        print("=" * 60)
        print("Initializing Real-time Hindi Voice Assistant")
        print("High-Speed Optimization")
        print("=" * 60)
        
        self.RATE = 16000
        self.CHUNK = 480 
        self.FORMAT = pyaudio.paInt16
        self.CHANNELS = 1
        
        self.vad = webrtcvad.Vad(2) 
        self.silence_threshold = 1.0 
        self.min_speech_duration = 0.5 
        self.max_recording_duration = 6.0  # Shorter = less noise accumulation
        
        self.audio = pyaudio.PyAudio()
        
        
        # Layer 1: ASR Loading (Faster-Whisper with Fallback)
        try:
            from faster_whisper import WhisperModel
            print("\n[Layer 1] Loading Faster-Whisper (Base, Int8 quantized)...")
            self.asr_model = WhisperModel(
                "base",                    
                device="cpu",               # CPU inference
                compute_type="int8",        # 8-bit quantization (Speed boost)
                cpu_threads=2,              # Only A76 cores (faster)
                num_workers=1               # Single worker for stability
            )
            self.use_faster_whisper = True
            print("‚úì Faster-Whisper loaded (optimized for SBC)")
        except Exception as e:
            print(f"‚ö†Ô∏è  Faster-Whisper failed: {e}")
            print("   Falling back to standard Whisper (will be slower)")
            import whisper
            self.asr_standard = whisper.load_model("base")
            self.use_faster_whisper = False
            
        self.TEMP_WAV = "temp_input.wav"
        
        # Layer 2: Advanced Grammar Corrector
        print("\n[Layer 2] Initializing Advanced Grammar Corrector...")
        self.corrector = AdvancedGrammarCorrector()
        
        # Layer 3: Robust Intent Classifier
        print("\n[Layer 3] Loading Robust Intent Classifier...")
        self.intent_classifier = RobustIntentClassifier()
        
        # TTS Settings
        script_dir = os.path.dirname(os.path.abspath(__file__))
        self.piper_model = os.path.join(script_dir, "models/hindi/hi_IN-rohan-medium.onnx")
        self.piper_sample_rate = 22050
        
        # Pre-cache ALL static responses for instant playback (Parallel)
        print("\n[TTS] Pre-generating all static responses (Parallel 2-workers)...")
        self.audio_cache = {}
        
        # Static responses that never change - Normalized to NFC
        raw_responses = [
            "‡§†‡•Ä‡§ï ‡§π‡•à, ‡§¨‡§Ç‡§¶ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Ç‡•§",
            "‡§®‡§Æ‡§∏‡•ç‡§§‡•á! ‡§Æ‡•á‡§∞‡§æ ‡§®‡§æ‡§Æ ‡§≠‡§æ‡§∞‡§§ AI ‡§π‡•à, ‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡•Ä ‡§ï‡•à‡§∏‡•á ‡§Æ‡§¶‡§¶ ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ ‡§π‡•Ç‡§Ç?",
            "‡§Ü‡§™‡§ï‡§æ ‡§∏‡•ç‡§µ‡§æ‡§ó‡§§ ‡§π‡•à!",
            "‡§Æ‡•à‡§Ç ‡§ú‡•ã‡§ï ‡§∏‡•Å‡§®‡§æ ‡§∏‡§ï‡§§‡§æ ‡§π‡•Ç‡§Å, ‡§∏‡§Ç‡§ó‡•Ä‡§§ ‡§¨‡§ú‡§æ ‡§∏‡§ï‡§§‡§æ ‡§π‡•Ç‡§Å ‡§î‡§∞ ‡§®‡§æ‡§ö ‡§≠‡•Ä ‡§∏‡§ï‡§§‡§æ ‡§π‡•Ç‡§Å‡•§ ‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞ ‡§î‡§∞ ‡§Æ‡•å‡§∏‡§Æ ‡§Ö‡§≠‡•Ä ‡§ë‡§´‡§≤‡§æ‡§á‡§® ‡§π‡•à‡§Ç, ‡§≤‡•á‡§ï‡§ø‡§® ‡§Æ‡•à‡§Ç ‡§∏‡§Æ‡§Ø ‡§î‡§∞ ‡§§‡§æ‡§∞‡•Ä‡§ñ ‡§¨‡§§‡§æ ‡§∏‡§ï‡§§‡§æ ‡§π‡•Ç‡§Å‡•§ ‡§Ü‡§™ ‡§ï‡•ç‡§Ø‡§æ ‡§ú‡§æ‡§®‡§®‡§æ ‡§ö‡§æ‡§π‡§§‡•á ‡§π‡•à‡§Ç?",
            "‡§Æ‡§æ‡§´‡§º ‡§ï‡§∞‡•á‡§Ç, ‡§Æ‡•à‡§Ç ‡§∏‡§Æ‡§ù ‡§®‡§π‡•Ä‡§Ç ‡§™‡§æ‡§Ø‡§æ‡•§ ‡§ï‡•É‡§™‡§Ø‡§æ ‡§´‡§ø‡§∞ ‡§∏‡•á ‡§¨‡•ã‡§≤‡•á‡§Ç‡•§",
            "‡§Æ‡•å‡§∏‡§Æ ‡§ï‡•Ä ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à‡•§ ‡§Æ‡•à‡§Ç ‡§ë‡§´‡§≤‡§æ‡§á‡§® ‡§ï‡§æ‡§Æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•Ç‡§Ç‡•§ ‡§≤‡•á‡§ï‡§ø‡§® ‡§Ü‡§ú ‡§¶‡§ø‡§® ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§≤‡§ó ‡§∞‡§π‡§æ ‡§π‡•à!",
            "‡§ó‡§æ‡§®‡§æ ‡§¨‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Ç... ‡§ß‡•Å‡§® ‡§ß‡•Å‡§® ‡§ß‡•Å! ‡§µ‡•à‡§∏‡•á ‡§Æ‡•à‡§Ç ‡§Ö‡§≠‡•Ä ‡§∏‡•ç‡§™‡•Ä‡§ï‡§∞ ‡§∏‡•á ‡§ú‡•Å‡§°‡§º‡§æ ‡§®‡§π‡•Ä‡§Ç ‡§π‡•Ç‡§Ç‡•§",
            "‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞ ‡§∏‡•á‡§µ‡§æ ‡§ë‡§´‡§≤‡§æ‡§á‡§® ‡§π‡•à‡•§ ‡§≤‡•á‡§ï‡§ø‡§® ‡§Ü‡§ú ‡§ï‡§æ ‡§¶‡§ø‡§® ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à!",
            "‡§Æ‡•à‡§Ç ‡§®‡§æ‡§ö ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Ç... ‡§ß‡§ø‡§® ‡§ß‡§ø‡§® ‡§ß‡§æ! ‡§≤‡•á‡§ï‡§ø‡§® ‡§Æ‡•á‡§∞‡•á ‡§™‡§æ‡§∏ ‡§™‡•à‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à‡§Ç!",
            "‡§è‡§ï ‡§∞‡•ã‡§¨‡•ã‡§ü ‡§°‡•â‡§ï‡•ç‡§ü‡§∞ ‡§ï‡•á ‡§™‡§æ‡§∏ ‡§ó‡§Ø‡§æ‡•§ ‡§°‡•â‡§ï‡•ç‡§ü‡§∞ ‡§¨‡•ã‡§≤‡§æ: ‡§Ü‡§™ ‡§§‡•ã ‡§¨‡§ø‡§≤‡•ç‡§ï‡•Å‡§≤ ‡§´‡§ø‡§ü ‡§π‡•à‡§Ç... ‡§¨‡§∏ ‡§•‡•ã‡§°‡§º‡§æ ‡§ë‡§Ø‡§≤ ‡§ö‡§æ‡§π‡§ø‡§è!",
            "‡§Æ‡•á‡§∞‡§æ ‡§è‡§ï ‡§¶‡•ã‡§∏‡•ç‡§§ ‡§π‡•à, ‡§µ‡§π ‡§≠‡•Ä AI ‡§π‡•à‡•§ ‡§π‡§Æ ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§¨‡§π‡•Å‡§§ ‡§∏‡•ç‡§Æ‡§æ‡§∞‡•ç‡§ü ‡§π‡•à‡§Ç!",
            "‡§Æ‡§ú‡§æ‡§ï: ‡§Æ‡•à‡§Ç‡§®‡•á ‡§è‡§ï ‡§¨‡§æ‡§∞ ‡§ï‡§π‡§æ ‡§•‡§æ ‡§Æ‡•à‡§Ç ‡§ë‡§´‡§≤‡§æ‡§á‡§® ‡§π‡•Ç‡§Ç, ‡§≤‡•á‡§ï‡§ø‡§® ‡§ï‡•ã‡§à ‡§Æ‡§æ‡§® ‡§π‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§∞‡§π‡§æ ‡§•‡§æ!"
        ]
        
        # Normalize all phrases to NFC for consistent matching
        common_responses = [unicodedata.normalize('NFC', p) for p in raw_responses]
        
        def cache_audio(phrase):
            try:
                # Use absolute path to python if needed, but sys.executable is usually right
                process = subprocess.Popen(
                    [sys.executable, '-m', 'piper', '--model', self.piper_model, '--output-raw'],
                    stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE
                )
                # Increased timeout for SBC stability
                audio_data, stderr_data = process.communicate(input=phrase.encode('utf-8'), timeout=30)
                
                if process.returncode != 0:
                    return phrase, None, f"Piper exited with code {process.returncode}: {stderr_data.decode()}"
                
                return phrase, audio_data, None
            except subprocess.TimeoutExpired:
                return phrase, None, "Timeout (30s) expired during generation"
            except Exception as e:
                return phrase, None, str(e)

        # Use only 2 workers to avoid CPU/RAM starvation on SBC
        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
            results = list(executor.map(cache_audio, common_responses))
            for phrase, audio, error in results:
                if audio:
                    self.audio_cache[phrase] = audio
                    # print(f"   ‚úì Cached (len={len(audio)}): {phrase}")
                else:
                    print(f"   ‚ö†Ô∏è Failed to cache '{phrase[:20]}...': {error}")
        
        print(f"   ‚úì Successfully cached {len(self.audio_cache)}/{len(common_responses)} responses (~{len(self.audio_cache) * 0.05:.1f}MB RAM)")
        
        self.HINDI_MONTHS = {
            'January': '‡§ú‡§®‡§µ‡§∞‡•Ä', 'February': '‡§´‡§º‡§∞‡§µ‡§∞‡•Ä', 'March': '‡§Æ‡§æ‡§∞‡•ç‡§ö',
            'April': '‡§Ö‡§™‡•ç‡§∞‡•à‡§≤', 'May': '‡§Æ‡§à', 'June': '‡§ú‡•Ç‡§®',
            'July': '‡§ú‡•Å‡§≤‡§æ‡§à', 'August': '‡§Ö‡§ó‡§∏‡•ç‡§§', 'September': '‡§∏‡§ø‡§§‡§Ç‡§¨‡§∞',
            'October': '‡§Ö‡§ï‡•ç‡§ü‡•Ç‡§¨‡§∞', 'November': '‡§®‡§µ‡§Ç‡§¨‡§∞', 'December': '‡§¶‡§ø‡§∏‡§Ç‡§¨‡§∞'
        }
        
        gc.collect() # Clean up after model loading
        print("\n‚úì All systems ready!\n")

    def record_with_vad(self):
        print("\nüé§ Listening... (speak now)")
        stream = self.audio.open(format=self.FORMAT, channels=self.CHANNELS,
                               rate=self.RATE, input=True,
                               frames_per_buffer=self.CHUNK)
        
        frames = []
        ring_buffer = collections.deque(maxlen=10)
        triggered = False
        silence_frames = 0
        start_time = time.time()
        speech_start = 0
        
        while True:
            frame = stream.read(self.CHUNK, exception_on_overflow=False)
            is_speech = self.vad.is_speech(frame, self.RATE)
            
            if not triggered:
                ring_buffer.append((frame, is_speech))
                num_voiced = len([f for f, s in ring_buffer if s])
                if num_voiced > 0.6 * ring_buffer.maxlen:
                    triggered = True
                    print("üî¥ Recording...")
                    speech_start = time.time()
                    for f, s in ring_buffer: frames.append(f)
                    ring_buffer.clear()
            else:
                frames.append(frame)
                if not is_speech:
                    silence_frames += 1
                else:
                    silence_frames = 0
                
                curr_time = time.time()
                silence_dur = (silence_frames * self.CHUNK) / self.RATE
                if silence_dur >= self.silence_threshold:
                    print("‚è∏Ô∏è  Silence detected, processing...")
                    break
                if (curr_time - start_time) > self.max_recording_duration:
                    print("‚è∏Ô∏è  Max duration reached, processing...")
                    break
                    
        stream.stop_stream()
        stream.close()
        
        duration = time.time() - speech_start
        if triggered and duration >= self.min_speech_duration:
            with wave.open(self.TEMP_WAV, 'wb') as wf:
                wf.setnchannels(self.CHANNELS)
                wf.setsampwidth(self.audio.get_sample_size(self.FORMAT))
                wf.setframerate(self.RATE)
                wf.writeframes(b''.join(frames))
            return True
        return False

    def generate_response(self, intent):
        now = datetime.now()
        response = ""
        
        if intent == "time":
            response = f"‡§Ö‡§≠‡•Ä ‡§∏‡§Æ‡§Ø ‡§π‡•à {now.strftime('%I:%M %p')}"
        elif intent == "date":
            month_hindi = self.HINDI_MONTHS.get(now.strftime('%B'), now.strftime('%B'))
            response = f"‡§Ü‡§ú ‡§ï‡•Ä ‡§§‡§æ‡§∞‡•Ä‡§ñ ‡§π‡•à {now.day} {month_hindi} {now.year}"
        elif intent == "hello":
            response = "‡§®‡§Æ‡§∏‡•ç‡§§‡•á! ‡§Æ‡•á‡§∞‡§æ ‡§®‡§æ‡§Æ ‡§≠‡§æ‡§∞‡§§ AI ‡§π‡•à, ‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡•Ä ‡§ï‡•à‡§∏‡•á ‡§Æ‡§¶‡§¶ ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ ‡§π‡•Ç‡§Ç?"
        elif intent == "thank_you":
            response = "‡§Ü‡§™‡§ï‡§æ ‡§∏‡•ç‡§µ‡§æ‡§ó‡§§ ‡§π‡•à!"
        elif intent == "help":
            response = "‡§Æ‡•à‡§Ç ‡§ú‡•ã‡§ï ‡§∏‡•Å‡§®‡§æ ‡§∏‡§ï‡§§‡§æ ‡§π‡•Ç‡§Å, ‡§∏‡§Ç‡§ó‡•Ä‡§§ ‡§¨‡§ú‡§æ ‡§∏‡§ï‡§§‡§æ ‡§π‡•Ç‡§Å ‡§î‡§∞ ‡§®‡§æ‡§ö ‡§≠‡•Ä ‡§∏‡§ï‡§§‡§æ ‡§π‡•Ç‡§Å‡•§ ‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞ ‡§î‡§∞ ‡§Æ‡•å‡§∏‡§Æ ‡§Ö‡§≠‡•Ä ‡§ë‡§´‡§≤‡§æ‡§á‡§® ‡§π‡•à‡§Ç, ‡§≤‡•á‡§ï‡§ø‡§® ‡§Æ‡•à‡§Ç ‡§∏‡§Æ‡§Ø ‡§î‡§∞ ‡§§‡§æ‡§∞‡•Ä‡§ñ ‡§¨‡§§‡§æ ‡§∏‡§ï‡§§‡§æ ‡§π‡•Ç‡§Å‡•§ ‡§Ü‡§™ ‡§ï‡•ç‡§Ø‡§æ ‡§ú‡§æ‡§®‡§®‡§æ ‡§ö‡§æ‡§π‡§§‡•á ‡§π‡•à‡§Ç?"
        elif intent == "stop":
            response = "‡§†‡•Ä‡§ï ‡§π‡•à, ‡§¨‡§Ç‡§¶ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Ç‡•§"
        elif intent == 'dance':
            response = "‡§Æ‡•à‡§Ç ‡§®‡§æ‡§ö ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Ç... ‡§ß‡§ø‡§® ‡§ß‡§ø‡§® ‡§ß‡§æ! ‡§≤‡•á‡§ï‡§ø‡§® ‡§Æ‡•á‡§∞‡•á ‡§™‡§æ‡§∏ ‡§™‡•à‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à‡§Ç!"
        elif intent == 'weather':
            response = "‡§Æ‡•å‡§∏‡§Æ ‡§ï‡•Ä ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à‡•§ ‡§Æ‡•à‡§Ç ‡§ë‡§´‡§≤‡§æ‡§á‡§® ‡§ï‡§æ‡§Æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•Ç‡§Ç‡•§ ‡§≤‡•á‡§ï‡§ø‡§® ‡§Ü‡§ú ‡§¶‡§ø‡§® ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§≤‡§ó ‡§∞‡§π‡§æ ‡§π‡•à!"
        elif intent == 'joke':
            if not hasattr(self, '_joke_index'):
                self._joke_index = 0
            
            jokes = [
                "‡§è‡§ï ‡§∞‡•ã‡§¨‡•ã‡§ü ‡§°‡•â‡§ï‡•ç‡§ü‡§∞ ‡§ï‡•á ‡§™‡§æ‡§∏ ‡§ó‡§Ø‡§æ‡•§ ‡§°‡•â‡§ï‡•ç‡§ü‡§∞ ‡§¨‡•ã‡§≤‡§æ: ‡§Ü‡§™ ‡§§‡•ã ‡§¨‡§ø‡§≤‡•ç‡§ï‡•Å‡§≤ ‡§´‡§ø‡§ü ‡§π‡•à‡§Ç... ‡§¨‡§∏ ‡§•‡•ã‡§°‡§º‡§æ ‡§ë‡§Ø‡§≤ ‡§ö‡§æ‡§π‡§ø‡§è!",
                "‡§Æ‡•á‡§∞‡§æ ‡§è‡§ï ‡§¶‡•ã‡§∏‡•ç‡§§ ‡§π‡•à, ‡§µ‡§π ‡§≠‡•Ä AI ‡§π‡•à‡•§ ‡§π‡§Æ ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§¨‡§π‡•Å‡§§ ‡§∏‡•ç‡§Æ‡§æ‡§∞‡•ç‡§ü ‡§π‡•à‡§Ç!",
                "‡§Æ‡§ú‡§æ‡§ï: ‡§Æ‡•à‡§Ç‡§®‡•á ‡§è‡§ï ‡§¨‡§æ‡§∞ ‡§ï‡§π‡§æ ‡§•‡§æ ‡§Æ‡•à‡§Ç ‡§ë‡§´‡§≤‡§æ‡§á‡§® ‡§π‡•Ç‡§Ç, ‡§≤‡•á‡§ï‡§ø‡§® ‡§ï‡•ã‡§à ‡§Æ‡§æ‡§® ‡§π‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§∞‡§π‡§æ ‡§•‡§æ!"
            ]
            response = jokes[self._joke_index % len(jokes)]
            self._joke_index += 1
        elif intent == 'music':
            response = "‡§ó‡§æ‡§®‡§æ ‡§¨‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Ç... ‡§ß‡•Å‡§® ‡§ß‡•Å‡§® ‡§ß‡•Å! ‡§µ‡•à‡§∏‡•á ‡§Æ‡•à‡§Ç ‡§Ö‡§≠‡•Ä ‡§∏‡•ç‡§™‡•Ä‡§ï‡§∞ ‡§∏‡•á ‡§ú‡•Å‡§°‡§º‡§æ ‡§®‡§π‡•Ä‡§Ç ‡§π‡•Ç‡§Ç‡•§"
        elif intent == 'news':
            response = "‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞ ‡§∏‡•á‡§µ‡§æ ‡§ë‡§´‡§≤‡§æ‡§á‡§® ‡§π‡•à‡•§ ‡§≤‡•á‡§ï‡§ø‡§® ‡§Ü‡§ú ‡§ï‡§æ ‡§¶‡§ø‡§® ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à!"
        else:
            response = "‡§Æ‡§æ‡§´‡§º ‡§ï‡§∞‡•á‡§Ç, ‡§Æ‡•à‡§Ç ‡§∏‡§Æ‡§ù ‡§®‡§π‡•Ä‡§Ç ‡§™‡§æ‡§Ø‡§æ‡•§ ‡§ï‡•É‡§™‡§Ø‡§æ ‡§´‡§ø‡§∞ ‡§∏‡•á ‡§¨‡•ã‡§≤‡•á‡§Ç‡•§"
        
        # CRITICAL: Normalize to NFC before returning to ensure cache hit
        return unicodedata.normalize('NFC', response)

    def speak(self, text):
        print(f"üîä Speaking (Natural Voice)...")
        start_tts = time.time()
        
        # Check cache first for instant playback
        # Normalize input text to NFC for consistent matching
        norm_text = unicodedata.normalize('NFC', text)
        
        if hasattr(self, 'audio_cache') and norm_text in self.audio_cache:
            print(f"   ‚úì Using cached audio (0.0s)")
            audio_data = self.audio_cache[norm_text]
            
            # Play cached audio immediately
            p = pyaudio.PyAudio()
            stream = p.open(format=pyaudio.paInt16, channels=1, 
                            rate=self.piper_sample_rate, output=True,
                            frames_per_buffer=256)
            stream.write(audio_data)
            stream.stop_stream()
            stream.close()
            p.terminate()
            
            total_time = time.time() - start_tts
            print(f"   Total latency: {total_time:.2f}s")
            return
        
        # If not cached, generate fresh audio
        print(f"   Generating fresh audio...")
        
        if os.path.exists(self.piper_model):
            try:
                process = subprocess.Popen(
                    [sys.executable, '-m', 'piper', '--model', self.piper_model, '--output-raw'],
                    stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE
                )
                audio_data, stderr_data = process.communicate(input=text.encode('utf-8'), timeout=10)
                
                # Show Piper errors if any
                if process.returncode != 0:
                    error_msg = stderr_data.decode()[:200] if stderr_data else "Unknown error"
                    print(f"   ‚ö†Ô∏è Piper failed: {error_msg}")
                    raise Exception("Piper TTS failed")
                
                if audio_data:
                    p = pyaudio.PyAudio()
                    stream = p.open(format=pyaudio.paInt16, channels=1, 
                                    rate=self.piper_sample_rate, output=True,
                                    frames_per_buffer=1024)
                    stream.write(audio_data)
                    stream.stop_stream()
                    stream.close()
                    p.terminate()
                    
                    total_time = time.time() - start_tts
                    print(f"   Total latency: {total_time:.2f}s")
                    return
            except subprocess.TimeoutExpired:
                print("   ‚ö†Ô∏è  Piper timeout, using fallback")
                process.kill()
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Piper failed: {e}")
        
        # Fallback to eSpeak
        subprocess.run(['espeak-ng', '-v', 'hi', '-s', '150', text], check=False)

    def run(self):
        try:
            while True:
                if self.record_with_vad():
                    if self.use_faster_whisper:
                        # Transcribe using faster-whisper (SPEED-OPTIMIZED)
                        segments, info = self.asr_model.transcribe(
                            self.TEMP_WAV,
                            beam_size=3,
                            language="hi",
                            task="transcribe",
                            initial_prompt="‡§Ø‡§π ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§µ‡•â‡§Ø‡§∏ ‡§Ö‡§∏‡§ø‡§∏‡•ç‡§ü‡•á‡§Ç‡§ü ‡§π‡•à‡•§ ‡§¨‡§Ç‡§¶ ‡§ï‡§∞‡•ã‡•§ ‡§¨‡§Ç‡§¶ ‡§π‡•ã ‡§ú‡§æ‡§ì‡•§ ‡§∏‡§Æ‡§Ø ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à‡•§ ‡§Ü‡§ú ‡§ï‡•å‡§® ‡§∏‡§æ ‡§¶‡§ø‡§® ‡§π‡•à‡•§ ‡§ó‡§æ‡§®‡§æ ‡§∏‡•Å‡§®‡§æ‡§ì‡•§ ‡§Æ‡§ú‡§æ‡§ï ‡§∏‡•Å‡§®‡§æ‡§ì‡•§ ‡§Æ‡•å‡§∏‡§Æ ‡§¨‡§§‡§æ‡§ì‡•§ ‡§®‡§Æ‡§∏‡•ç‡§§‡•á‡•§ ‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶‡•§ ‡§∂‡•Å‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ‡•§",
                            vad_filter=True,
                            condition_on_previous_text=False,
                            best_of=1,
                            temperature=0.0,
                            compression_ratio_threshold=2.4,
                            log_prob_threshold=-1.0,
                            no_speech_threshold=0.6
                        )
                        
                        # Check if Hindi was detected
                        if info.language != "hi":
                            print(f"‚ö†Ô∏è  Wrong language: {info.language} (prob: {info.language_probability:.0%})")
                            print(f"   Forcing Hindi retry...")
                            segments, info = self.asr_model.transcribe(
                                self.TEMP_WAV,
                                language="hi",
                                task="transcribe",
                                beam_size=5,
                                initial_prompt="‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§π‡§ø‡§Ç‡§¶‡•Ä‡•§ ‡§¨‡§Ç‡§¶ ‡§ï‡§∞‡•ã‡•§ ‡§¨‡§Ç‡§¶ ‡§π‡•ã ‡§ú‡§æ‡§ì‡•§ ‡§∏‡§Æ‡§Ø ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à‡•§ ‡§ó‡§æ‡§®‡§æ ‡§∏‡•Å‡§®‡§æ‡§ì‡•§ ‡§Æ‡§ú‡§æ‡§ï ‡§∏‡•Å‡§®‡§æ‡§ì‡•§"
                            )

                        raw_text = " ".join([segment.text for segment in segments]).strip()
                    else:
                        # Fallback to standard whisper
                        result = self.asr_standard.transcribe(self.TEMP_WAV, language="hi", fp16=False)
                        raw_text = result['text'].strip()
                        
                    print(f"üìù Raw transcription: '{raw_text}'")
                    
                    corrected = self.corrector.correct(raw_text)
                    
                    intent, conf = self.intent_classifier.classify(corrected)
                    print(f"üéØ Intent: {intent} (confidence: {conf:.1%})")
                    
                    response = self.generate_response(intent)
                    
                    print(f"üí¨ Response: {response}")
                    self.speak(response)
                    
                    # Exit commands (no timeout condition)
                    if intent == "stop":
                        print("\nüëã Goodbye!")
                        break
                    print("-" * 60)
        except KeyboardInterrupt:
            print("\nüëã Stopped by user")
        finally:
            if os.path.exists(self.TEMP_WAV): os.remove(self.TEMP_WAV)
            self.audio.terminate()

if __name__ == "__main__":
    assistant = RealtimeVoiceAssistant()
    assistant.run()
